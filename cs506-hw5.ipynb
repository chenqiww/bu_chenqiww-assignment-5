{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define the KNN class with flexible options\n",
        "class KNN:\n",
        "    def __init__(self, k=15, distance_metric='euclidean', weighting='uniform'):\n",
        "        self.k = k\n",
        "        self.distance_metric = distance_metric\n",
        "        self.weighting = weighting  # 'uniform', 'inverse_distance'\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        # Compute class priors\n",
        "        self.class_priors = {\n",
        "            0: np.mean(y == 0),\n",
        "            1: np.mean(y == 1)\n",
        "        }\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        proba = []\n",
        "        for x in X:\n",
        "            distances = self.compute_distance(self.X_train, x)\n",
        "            k_indices = distances.argsort()[:self.k]\n",
        "            k_nearest_labels = self.y_train[k_indices]\n",
        "            k_nearest_distances = distances[k_indices]\n",
        "\n",
        "            if self.weighting == 'uniform':\n",
        "                # Simple average\n",
        "                prob = np.mean(k_nearest_labels)\n",
        "            elif self.weighting == 'inverse_distance':\n",
        "                # Avoid division by zero\n",
        "                weights = 1 / (k_nearest_distances + 1e-5)\n",
        "                prob = np.sum(weights * k_nearest_labels) / np.sum(weights)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported weighting scheme\")\n",
        "            proba.append(prob)\n",
        "        return np.array(proba)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= threshold).astype(int)\n",
        "\n",
        "    def compute_distance(self, X, x):\n",
        "        if self.distance_metric == 'euclidean':\n",
        "            distances = np.sqrt(np.sum((X - x) ** 2, axis=1))\n",
        "        elif self.distance_metric == 'manhattan':\n",
        "            distances = np.sum(np.abs(X - x), axis=1)\n",
        "        elif self.distance_metric == 'cosine':\n",
        "            # Cosine distance\n",
        "            dot_product = np.sum(X * x, axis=1)\n",
        "            norm_x = np.linalg.norm(x)\n",
        "            norm_X = np.linalg.norm(X, axis=1)\n",
        "            distances = 1 - (dot_product / (norm_X * norm_x + 1e-5))\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported distance metric\")\n",
        "        return distances\n",
        "\n",
        "# Function to compute ROC AUC manually\n",
        "def compute_roc_auc(y_true, y_scores, num_thresholds=100):\n",
        "    thresholds = np.linspace(0, 1, num_thresholds)\n",
        "    tpr_list = []\n",
        "    fpr_list = []\n",
        "    for thresh in thresholds:\n",
        "        y_pred = (y_scores >= thresh).astype(int)\n",
        "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Sensitivity\n",
        "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # 1 - Specificity\n",
        "        tpr_list.append(tpr)\n",
        "        fpr_list.append(fpr)\n",
        "    # Sort false positive rates and true positive rates\n",
        "    fpr_list, tpr_list = zip(*sorted(zip(fpr_list, tpr_list)))\n",
        "    # Compute AUC using the trapezoidal rule\n",
        "    roc_auc = np.trapz(tpr_list, fpr_list)\n",
        "    return roc_auc\n",
        "\n",
        "# Function to generate parameter combinations without itertools\n",
        "def generate_param_combinations(param_grid):\n",
        "    keys = list(param_grid.keys())\n",
        "    values = list(param_grid.values())\n",
        "    combinations = [[]]\n",
        "    for value_list in values:\n",
        "        combinations = [x+[y] for x in combinations for y in value_list]\n",
        "    param_dicts = [dict(zip(keys, combination)) for combination in combinations]\n",
        "    return param_dicts\n",
        "\n",
        "# Define data preprocessing function with options\n",
        "def preprocess_data(train_path, test_path, scaling_method='standard', feature_engineering=True, features_to_drop=None):\n",
        "    train_data = pd.read_csv(train_path)\n",
        "    test_data = pd.read_csv(test_path)\n",
        "\n",
        "    # Combine train and test data for consistent preprocessing\n",
        "    data = pd.concat([train_data, test_data], sort=False).reset_index(drop=True)\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    data.drop(['CustomerId', 'Surname'], axis=1, inplace=True)\n",
        "\n",
        "    # Handle missing values in 'Gender' and 'Geography'\n",
        "    data['Gender'] = data['Gender'].fillna('Unknown')\n",
        "    data['Geography'] = data['Geography'].fillna('Unknown')\n",
        "\n",
        "    # Map 'Gender' values to numeric\n",
        "    gender_mapping = {'Male': 0, 'Female': 1, 'Unknown': 2}\n",
        "    data['Gender'] = data['Gender'].map(gender_mapping)\n",
        "\n",
        "    # One-hot encode 'Geography'\n",
        "    geography_dummies = pd.get_dummies(data['Geography'], prefix='Geography')\n",
        "    data = pd.concat([data, geography_dummies], axis=1)\n",
        "    data.drop('Geography', axis=1, inplace=True)\n",
        "    geography_columns = geography_dummies.columns.tolist()\n",
        "    data[geography_columns] = data[geography_columns].astype(int)\n",
        "\n",
        "    # Convert numeric columns to numeric data types\n",
        "    numeric_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts',\n",
        "                    'EstimatedSalary']\n",
        "    for col in numeric_cols:\n",
        "        if col in data.columns:\n",
        "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "    # Handle missing values in numeric columns\n",
        "    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
        "\n",
        "    # Convert boolean columns to integers\n",
        "    boolean_cols = ['HasCrCard', 'IsActiveMember']\n",
        "    for col in boolean_cols:\n",
        "        if col in data.columns:\n",
        "            data[col] = data[col].astype(int)\n",
        "\n",
        "    # Ensure 'Exited' is integer type\n",
        "    if 'Exited' in data.columns:\n",
        "        data['Exited'] = data['Exited'].fillna(0).astype(int)\n",
        "\n",
        "    # Feature Engineering\n",
        "    if feature_engineering:\n",
        "        data['Age_CreditScore_Ratio'] = data['Age'] / (data['CreditScore'] + 1e-5)\n",
        "        data['Balance_Salary_Ratio'] = data['Balance'] / (data['EstimatedSalary'] + 1e-5)\n",
        "        data['IsYoung'] = (data['Age'] < 30).astype(int)\n",
        "        data['IsSenior'] = (data['Age'] > 60).astype(int)\n",
        "        data['Tenure_By_Age'] = data['Tenure'] / (data['Age'] + 1e-5)\n",
        "\n",
        "    # Drop specified features\n",
        "    if features_to_drop:\n",
        "        data.drop(features_to_drop, axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "    # Prepare features to scale\n",
        "    features_to_scale = data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    features_to_scale = [col for col in features_to_scale if col not in ['Exited', 'id']]\n",
        "\n",
        "    # Feature scaling\n",
        "    for feature in features_to_scale:\n",
        "        if scaling_method == 'minmax':\n",
        "            min_val = data[feature].min()\n",
        "            max_val = data[feature].max()\n",
        "            if max_val - min_val != 0:\n",
        "                data[feature] = (data[feature] - min_val) / (max_val - min_val)\n",
        "            else:\n",
        "                data[feature] = 0\n",
        "        elif scaling_method == 'standard':\n",
        "            mean_val = data[feature].mean()\n",
        "            std_val = data[feature].std()\n",
        "            if std_val != 0:\n",
        "                data[feature] = (data[feature] - mean_val) / std_val\n",
        "            else:\n",
        "                data[feature] = 0\n",
        "        elif scaling_method == 'none':\n",
        "            pass  # No scaling\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported scaling method\")\n",
        "\n",
        "    # Split data back into train and test sets\n",
        "    train = data[:len(train_data)].reset_index(drop=True)\n",
        "    test = data[len(train_data):].reset_index(drop=True)\n",
        "\n",
        "    X_train = train.drop(['Exited', 'id'], axis=1).values.astype(float)\n",
        "    y_train = train['Exited'].values.astype(int)\n",
        "    X_test = test.drop(['Exited', 'id'], axis=1).values.astype(float)\n",
        "\n",
        "    train_ids = train['id'].values if 'id' in train.columns else None\n",
        "    test_ids = test['id'].values if 'id' in test.columns else None\n",
        "\n",
        "    return X_train, y_train, X_test, train_ids, test_ids\n",
        "\n",
        "# Cross-validation function without external libraries\n",
        "def cross_validate(X, y, param_grid, n_splits=5):\n",
        "    results = []\n",
        "    n_samples = len(X)\n",
        "    indices = np.arange(n_samples)\n",
        "\n",
        "    # Stratify the folds based on class labels\n",
        "    class_indices = {}\n",
        "    for label in np.unique(y):\n",
        "        class_indices[label] = indices[y == label]\n",
        "\n",
        "    fold_indices = [[] for _ in range(n_splits)]\n",
        "    for label, idx_list in class_indices.items():\n",
        "        idx_list = idx_list.copy()\n",
        "        np.random.shuffle(idx_list)\n",
        "        folds = np.array_split(idx_list, n_splits)\n",
        "        for i in range(n_splits):\n",
        "            fold_indices[i].extend(folds[i])\n",
        "\n",
        "    # Create all combinations of parameters\n",
        "    all_params = generate_param_combinations(param_grid)\n",
        "    for param_dict in all_params:\n",
        "        scores = {'roc_auc': []}\n",
        "        for i in range(n_splits):\n",
        "            valid_idx = fold_indices[i]\n",
        "            train_idx = np.hstack([fold_indices[j] for j in range(n_splits) if j != i])\n",
        "\n",
        "            X_train_cv, y_train_cv = X[train_idx], y[train_idx]\n",
        "            X_valid_cv, y_valid_cv = X[valid_idx], y[valid_idx]\n",
        "\n",
        "            knn = KNN(\n",
        "                k=param_dict['k'],\n",
        "                distance_metric=param_dict['distance_metric'],\n",
        "                weighting=param_dict['weighting'],\n",
        "            )\n",
        "            knn.fit(X_train_cv, y_train_cv)\n",
        "            y_proba = knn.predict_proba(X_valid_cv)\n",
        "\n",
        "            # Compute ROC AUC manually\n",
        "            roc_auc = compute_roc_auc(y_valid_cv, y_proba)\n",
        "            scores['roc_auc'].append(roc_auc)\n",
        "        avg_scores = {metric_name: np.mean(scores[metric_name]) for metric_name in scores}\n",
        "        results.append({\n",
        "            **param_dict,\n",
        "            **avg_scores\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y, X_test, train_ids, test_ids = preprocess_data(\n",
        "    'train.csv', 'test.csv',\n",
        "    scaling_method='standard',\n",
        "    feature_engineering=True,\n",
        "    features_to_drop=None  # You can specify features to drop here\n",
        ")\n",
        "\n",
        "# Define parameter grid for cross-validation\n",
        "param_grid = {\n",
        "    'k': [5, 20,40],\n",
        "    'distance_metric': ['euclidean', 'manhattan'],\n",
        "    'weighting': ['uniform', 'inverse_distance'],\n",
        "}\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_results = cross_validate(X, y, param_grid, n_splits=5)\n",
        "\n",
        "# Now sort by 'roc_auc'\n",
        "print(\"\\nCross-validation results (sorted by ROC AUC):\")\n",
        "print(cv_results.sort_values(by='roc_auc', ascending=False).head(10))\n",
        "\n",
        "# Select the best parameters based on highest ROC AUC\n",
        "best_result = cv_results.sort_values(by='roc_auc', ascending=False).iloc[0]\n",
        "best_params = best_result.copy()\n",
        "print(f\"\\nBest parameters:\")\n",
        "for key, value in best_params.items():\n",
        "    if key != 'roc_auc':\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "# Train on full dataset with optimal hyperparameters and make predictions on test set\n",
        "knn = KNN(\n",
        "    k=int(best_params['k']),\n",
        "    distance_metric=best_params['distance_metric'],\n",
        "    weighting=best_params['weighting'],\n",
        ")\n",
        "knn.fit(X, y)\n",
        "test_proba = knn.predict_proba(X_test)\n",
        "\n",
        "# Save test predictions (probabilities)\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'Exited': test_proba  # Use probabilities instead of class labels\n",
        "})\n",
        "submission.to_csv('submissions.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buKEMOLYAKfv",
        "outputId": "968ccb5d-d66f-4096-b8ff-695599fde3d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-validation results (sorted by ROC AUC):\n",
            "     k distance_metric         weighting   roc_auc\n",
            "11  40       manhattan  inverse_distance  0.910006\n",
            "10  40       manhattan           uniform  0.908250\n",
            "9   40       euclidean  inverse_distance  0.906110\n",
            "7   20       manhattan  inverse_distance  0.905177\n",
            "8   40       euclidean           uniform  0.903672\n",
            "6   20       manhattan           uniform  0.903375\n",
            "5   20       euclidean  inverse_distance  0.902033\n",
            "4   20       euclidean           uniform  0.899811\n",
            "3    5       manhattan  inverse_distance  0.875166\n",
            "2    5       manhattan           uniform  0.873640\n",
            "\n",
            "Best parameters:\n",
            "k: 40\n",
            "distance_metric: manhattan\n",
            "weighting: inverse_distance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lHfPmo_oJUde"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}